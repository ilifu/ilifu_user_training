{"version": 2, "width": 106, "height": 29, "timestamp": 1771412047, "env": {"SHELL": "/usr/bin/zsh", "TERM": "xterm-256color"}}
[0.035621, "o", "\u001b[?2004h\u001b]0;dane@dane-latitude-7640: ~\u0007\u001b[01;32mdane@dane-latitude-7640\u001b[00m:\u001b[01;34m~\u001b[00m$ "]
[0.483043, "o", "s"]
[0.631424, "o", "s"]
[0.692886, "o", "h"]
[0.82195, "o", " "]
[0.990067, "o", "d"]
[1.078092, "o", "a"]
[1.124274, "o", "n"]
[1.179669, "o", "e"]
[1.466167, "o", "@"]
[1.762651, "o", "s"]
[1.835124, "o", "l"]
[2.029092, "o", "u"]
[2.152084, "o", "r"]
[2.217935, "o", "m"]
[2.458742, "o", "."]
[2.827101, "o", "i"]
[2.936708, "o", "l"]
[2.987303, "o", "i"]
[3.135109, "o", "f"]
[3.198198, "o", "u"]
[3.420764, "o", "."]
[3.516138, "o", "a"]
[3.608798, "o", "c"]
[3.711225, "o", "."]
[3.997492, "o", "z"]
[4.079017, "o", "a"]
[5.235376, "o", "\r\n\u001b[?2004l\r"]
[5.361082, "o", "Last login: Wed Feb 18 12:47:09 2026 from 41.164.33.145\r\r\n"]
[5.82594, "o", "\r\n+---------------------------------------------------------------------+\r\n|    \u001b[2m_ _ _  __      \u001b[0m                                                  |\r\n|   \u001b[2m(_) (_)/ _|_  _ \u001b[0m    Welcome to the ilifu SLURM Cluster            |\r\n|   \u001b[2m| | | |  _| || |\u001b[0m    Please familiarise yourself with the list     |\r\n|   \u001b[2m|_|_|_|_|  \\_,_|\u001b[0m    of recommendations below.                     |\r\n|                                                                     |\r\n| \u001b[1mDOs\u001b[0m:                                                                |\r\n| * try run jobs using sbatch rather than interactive jobs            |\r\n| * store larger files in a project directory or your                 |\r\n|   \u001b[1m~/workspace\u001b[0m directory (latter limited to 10 TiB total)            |\r\n| * cleanup unused files when not needed                              |\r\n| * set \u001b[1m--time\u001b[0m, \u001b[1m--mem\u001b[0m, \u001b[1m--account\u001b[0m parameters when submitting jobs,     |\r\n|   accurate description of job parameters improves the performance   |\r\n|   of the SLURM scheduler                                            |\r\n|                                                                     |\r\n| \u001b[1mDO NOTs\u001b[0m:                                                            |\r\n| * run software on the login-node                                    |\r\n| * transfer large data on the login-node, use \u001b[1mtransfer.ilifu.ac.za\u001b[0m   |\r\n|   (accessed via ssh) to do this                                     |\r\n| * copy large files to \u001b[1m/users\u001b[0m directory                              |\r\n| * leave data in \u001b[1m/scratch3\u001b[0m as this space is limited. After           |\r\n|   processing remove data that is not required and move files        |\r\n|   to your project directory                                         |\r\n|                                                                     |\r\n| User documentation is available at \u001b[1mhttps://docs.ilifu.ac.za/\u001b[0m        |\r\n|                                                                     |\r\n| For any queries or if you need help please contact the support team |\r\n| at \u001b[1msupport@ilifu.ac.za\u001b[0m                                              |\r\n|                                                                     |\r\n| Please login to \u001b[1mhttps://reports.ilifu.ac.za/\u001b[0m and make sure your     |\r\n| account is up to date as well as to view usage summaries.           |\r\n|                                                                     |\r\n+\u001b[2m---------------------------------------------------------------------\u001b[0m+\r\n\r\n"]
[6.070763, "o", "Valid Slurm Accounts for user \u001b[1mdane\u001b[0m on \u001b[1milifu-slurm2021\u001b[0m:\r\n  \u001b[1mb03-idia-ag\u001b[0m\r\n  \u001b[1mb34-admins-ag\u001b[0m  \u001b[2m(default)\u001b[0m\r\n  \u001b[1mb16-cbio-ag\u001b[0m\r\n  \u001b[1mb186-cbio-training-ag\u001b[0m\r\n  \u001b[1mb188-cbio-037-ag\u001b[0m\r\n  \u001b[1mb25-sanbi-group-ag\u001b[0m\r\n  \u001b[1mb73-ilifu-ag\u001b[0m\r\n  \u001b[1mb13-cchem-ag\u001b[0m\r\nChange your default account with:\r\n  sacctmgr modify user name=dane set DefaultAccount=\u001b[1m<account>\u001b[0m\r\nRunning job count: \u001b[1m0\u001b[0m\r\nPending job count: \u001b[1m0\u001b[0m\r\n\r\nRun the \"\u001b[1mshelp\u001b[0m\" command to display this message.\r\n\r\n"]
[6.114918, "o", "\u001b[?2004hdane@slurm-login:~$ "]
[6.840945, "o", "s"]
[7.116836, "o", "i"]
[7.182843, "o", "n"]
[7.295191, "o", "f"]
[7.596466, "o", "o"]
[8.280806, "o", "\r\n\u001b[?2004l\r"]
[8.290783, "o", "PARTITION  AVAIL  TIMELIMIT  NODES  STATE NODELIST\r\nMain*         up 14-00:00:0      6   mix- compute-[002,021,023,026,223,230]\r\nMain*         up 14-00:00:0      2   drng compute-[022,104]\r\nMain*         up 14-00:00:0      2  drain compute-[102-103]\r\n"]
[8.291211, "o", "Main*         up 14-00:00:0     23    mix compute-[003-004,006-008,010,014,016,025,101,105,207-208,214,217-218,220-221,228,242-243,245,254]\r\nMain*         up 14-00:00:0     53  alloc compute-[005,009,011-013,015,017-020,024,201-206,209-213,215-216,219,222,224-227,229,231-241,244,246-253,255-256]\r\nJupyter       up   infinite     13  alloc jupyter-[002-014]\r\nJupyterGPU    up 14-00:00:0      2    mix gpu-[003-004]\r\nJupyterDev    up   infinite      1  alloc jupyter-001\r\nHighMem       up 14-00:00:0      1   mix- highmem-002\r\nHighMem       up 14-00:00:0      2    mix highmem-[003,008]\r\nHighMem       up 14-00:00:0      5  alloc highmem-[001,004-007]\r\nGPU           up 14-00:00:0      4    mix gpu-[001-004]\r\nGPU           up 14-00:00:0      3   idle gpu-[005-007]\r\nDevel         up 5-00:00:00      1  alloc compute-001\r\n\u001b[?2004hdane@slurm-login:~$ "]
[10.089376, "o", "s"]
[10.234539, "o", "b"]
[10.328976, "o", "a"]
[10.529008, "o", "t"]
[11.192165, "o", "c"]
[11.253552, "o", "h"]
[12.435715, "o", " "]
[13.249835, "o", "m"]
[13.31704, "o", "y"]
[13.409092, "o", "\u0007_"]
[14.37201, "o", "j"]
[14.402096, "o", "o"]
[14.503243, "o", "b.sbatch"]
[16.072515, "o", "\r\n\u001b[?2004l\r"]
[16.151427, "o", "Submitted batch job 12832506\r\n\u001b[?2004hdane@slurm-login:~$ "]
[16.890799, "o", "s"]
[16.930918, "o", "q"]
[17.080532, "o", "u"]
[17.184159, "o", "e"]
[17.248801, "o", "u"]
[17.299825, "o", "e"]
[17.969844, "o", " "]
[19.376789, "o", "-"]
[20.003437, "o", "U"]
[20.156821, "o", " "]
[20.525913, "o", "$"]
[21.073529, "o", "U"]
[21.165505, "o", "S"]
[21.390397, "o", "E"]
[21.419454, "o", "R"]
[21.749908, "o", "\r\n\u001b[?2004l\r"]
[21.760653, "o", "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\r\n\u001b[?2004hdane@slurm-login:~$ "]
[25.633573, "o", "squeue -U $USER"]
[25.794104, "o", "\b"]
[26.246686, "o", "\b"]
[26.298506, "o", "\b"]
[26.316009, "o", "\b"]
[26.356211, "o", "\b"]
[26.38389, "o", "\b"]
[27.364052, "o", "\b\u001b[1P $USER\b\b\b\b\b\b"]
[27.380861, "o", "u $USER\b\b\b\b\b\b"]
[27.474838, "o", "\r\n\u001b[?2004l\r"]
[27.48875, "o", "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\r\n"]
[27.517835, "o", "          12832506      Main my_job.s     dane PD       0:00      1 (Priority)\r\n\u001b[?2004h"]
[27.971807, "o", "dane@slurm-login:~$ "]
[35.612497, "o", "squeue -u $USER"]
[37.217467, "o", "\r\n\u001b[?2004l\r"]
[37.225336, "o", "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\r\n          12832506      Main my_job.s     dane  R       0:08      1 compute-002\r\n\u001b[?2004hdane@slurm-login:~$ "]
[38.469762, "o", "s"]
[38.556285, "o", "c"]
[38.66468, "o", "a"]
[38.85198, "o", "n"]
[39.25439, "o", "c"]
[39.478562, "o", "e"]
[39.563887, "o", "l"]
[39.790277, "o", " "]
[43.542935, "o", "\u001b[7m12832506\u001b[27m"]
[44.038637, "o", "\b\b\b\b\b\b\b\b12832506\r\n\u001b[?2004l\r"]
[44.048255, "o", "\u001b[?2004hdane@slurm-login:~$ "]
[45.529655, "o", "scancel 12832506"]
[45.691115, "o", "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\u001b[1Pqueue -u $USER"]
[46.796589, "o", "\r\n\u001b[?2004l\r             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\r\n          12832506      Main my_job.s     dane CG       0:15      1 compute-002\r\n\u001b[?2004hdane@slurm-login:~$ "]
[49.41778, "o", "squeue -u $USER"]
[50.401517, "o", "\r\n\u001b[?2004l\r             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\r\n\u001b[?2004hdane@slurm-login:~$ "]
[53.796461, "o", "s"]
[53.972008, "o", "b"]
[54.042476, "o", "a"]
[54.245713, "o", "t"]
[54.515546, "o", "c"]
[54.58222, "o", "h"]
[55.008687, "o", " "]
[55.269699, "o", "-"]
[55.384541, "o", "-"]
[55.503627, "o", "h"]
[55.538653, "o", "e"]
[55.778761, "o", "lp"]
[56.776253, "o", "\r\n\u001b[?2004l\r"]
[56.781059, "o", "Usage: sbatch [OPTIONS(0)...] [ : [OPTIONS(N)...]] script(0) [args(0)...]\r\n\r\nParallel run options:\r\n  -a, --array=indexes         job array index values\r\n  -A, --account=name          charge job to specified account\r\n      --bb=<spec>             burst buffer specifications\r\n      --bbf=<file_name>       burst buffer specification file\r\n  -b, --begin=time            defer job until HH:MM MM/DD/YY\r\n      --comment=name          arbitrary comment\r\n      --cpu-freq=min[-max[:gov]] requested cpu frequency (and governor)\r\n  -c, --cpus-per-task=ncpus   number of cpus required per task\r\n  -d, --dependency=type:jobid[:time] defer job until condition on jobid is satisfied\r\n      --deadline=time         remove the job if no ending possible before\r\n                              this deadline (start > (deadline - time[-min]))\r\n      --delay-boot=mins       delay boot for desired node features\r\n  -D, --chdir=directory       set working directory for batch script\r\n  -e, --error=err             file for batch script's standard error\r\n      --export[=names]        specify environment variables to export\r\n      --export-file=file|fd   specify environment variables file or file\r\n                              descriptor to export\r\n      --get-user-env          load environment from local cluster\r\n      --gid=group_id          group ID to run job as (user root only)\r\n      --gres=list             required generic resources per node\r\n      --gres-flags=opts       flags related to GRES management\r\n  -H, --hold                  submit job in held state\r\n      --ignore-pbs            Ignore #PBS and #BSUB options in the batch script\r\n  -i, --input=in              file for batch script's standard input\r\n  -J, --job-name=jobname      name of job\r\n  -k, --no-kill               do not kill job on node failure\r\n  -L, --licenses=names        required license, comma separated\r\n  -M, --clusters=names        Comma separated list of clusters to issue\r\n                              commands to.  Default is current cluster.\r\n                              Name of 'all' will submit to run on all clusters.\r\n                              NOTE: SlurmDBD must up.\r\n      --container             Path to OCI container bundle\r\n      --container-id          OCI container ID\r\n  -m, --distribution=type     distribution method for processes to nodes\r\n                              (type = block|cyclic|arbitrary)\r\n      --mail-type=type        notify on state change: BEGIN, END, FAIL or ALL\r\n      --mail-user=user        who to send email notification for job state\r\n                              changes\r\n      --mcs-label=mcs         mcs label if mcs plugin mcs/group is used\r\n  -n, --ntasks=ntasks         number of tasks to run\r\n      --nice[=value]          decrease scheduling priority by value\r\n      --no-requeue            if set, do not permit the job to be requeued\r\n      --ntasks-per-node=n     number of tasks to invoke on each node\r\n  -N, --nodes=N               number of nodes on which to run (N = min[-max])\r\n      --oom-kill-step[=0|1]   set the OOMKillStep behaviour\r\n  -o, --output=out            file for batch script's standard output\r\n  -O, --overcommit            overcommit resources\r\n  -p, --partition=partition   partition requested\r\n      --parsable              outputs only the jobid and cluster name (if present),\r\n                              separated by semicolon, only on successful submission.\r\n      --power=flags           power management options\r\n      --priority=value        set the priority of the job to value\r\n      --profile=value         enable acct_gather_profile for detailed data\r\n                              value is all or none or any combination of\r\n                              energy, lustre, network or task\r\n      --propagate[=rlimits]   propagate all [or specific list of] rlimits\r\n  -q, --qos=qos               quality of service\r\n  -Q, --quiet                 quiet mode (suppress informational messages)\r\n      --reboot                reboot compute nodes before starting job\r\n      --requeue               if set, permit the job to be"]
[56.781208, "o", " requeued\r\n  -s, --oversubscribe         over subscribe resources with other jobs\r\n  -S, --core-spec=cores       count of reserved cores\r\n      --signal=[[R][B]:]num[@time] send signal when time limit within time seconds\r\n      --spread-job            spread job across as many nodes as possible\r\n      --switches=max-switches{@max-time-to-wait}\r\n                              Optimum switches and max time to wait for optimum\r\n      --thread-spec=threads   count of reserved threads\r\n  -t, --time=minutes          time limit\r\n      --time-min=minutes      minimum time limit (if distinct)\r\n      --tres-bind=...         task to tres binding options\r\n      --tres-per-task=list    list of tres required per task\r\n      --uid=user_id           user ID to run job as (user root only)\r\n      --use-min-nodes         if a range of node counts is given, prefer the\r\n                              smaller count\r\n  -v, --verbose               verbose mode (multiple -v's increase verbosity)\r\n  -W, --wait                  wait for completion of submitted job\r\n      --wckey=wckey           wckey to run job under\r\n      --wrap[=command string] wrap command string in a sh script and submit\r\n\r\nConstraint options:\r\n      --cluster-constraint=[!]list specify a list of cluster constraints\r\n      --contiguous            demand a contiguous range of nodes\r\n  -C, --constraint=list       specify a list of constraints\r\n  -F, --nodefile=filename     request a specific list of hosts\r\n      --mem=MB                minimum amount of real memory\r\n      --mincpus=n             minimum number of logical processors (threads)\r\n                              per node\r\n      --reservation=name      allocate resources from named reservation\r\n      --tmp=MB                minimum amount of temporary disk\r\n  -w, --nodelist=hosts...     request a specific list of hosts\r\n  -x, --exclude=hosts...      exclude a specific list of hosts\r\n\r\nConsumable resources related options:\r\n      --exclusive[=user]      allocate nodes in exclusive mode when\r\n                              cpu consumable resource is enabled\r\n      --exclusive[=mcs]       allocate nodes in exclusive mode when\r\n                              cpu consumable resource is enabled\r\n                              and mcs plugin is enabled\r\n      --mem-per-cpu=MB        maximum amount of real memory per allocated\r\n                              cpu required by the job.\r\n                              --mem >= --mem-per-cpu if --mem is specified.\r\n      --resv-ports            reserve communication ports\r\n\r\nAffinity/Multi-core options: (when the task/affinity plugin is enabled)\r\n                              For the following 4 options, you are\r\n                              specifying the minimum resources available for\r\n                              the node(s) allocated to the job.\r\n      --sockets-per-node=S    number of sockets per node to allocate\r\n      --cores-per-socket=C    number of cores per socket to allocate\r\n      --threads-per-core=T    number of threads per core to allocate\r\n  -B, --extra-node-info=S[:C[:T]]  combine request of sockets per node,\r\n                              cores per socket and threads per core.\r\n                              Specify an asterisk (*) as a placeholder,\r\n                              a minimum value, or a min-max range.\r\n\r\n      --ntasks-per-core=n     number of tasks to invoke on each core\r\n      --ntasks-per-socket=n   number of tasks to invoke on each socket\r\n\r\nGPU scheduling options:\r\n      --cpus-per-gpu=n        number of CPUs required per allocated GPU\r\n  -G, --gpus=n                count of GPUs required for the job\r\n      --gpu-bind=...          task to gpu binding options\r\n      --gpu-freq=...          frequency and voltage of GPUs\r\n      --gpus-per-node=n       number of GPUs required per allocated node\r\n      --gpus-per-socket=n     number of GPUs required per allocated socket\r\n      --gpus-per-task=n       number of GPUs required per spawned task\r\n      --mem-per-gpu=n         real memory required per allocated GPU\r\n\r\nHelp options:\r\n  -h, --help             "]
[56.781306, "o", "     show this help message\r\n      --usage                 display brief usage message\r\n\r\nOther options:\r\n  -V, --version               output version information and exit\r\n\r\n\u001b[?2004hdane@slurm-login:~$ "]
[58.577535, "o", "m"]
[58.642271, "o", "a"]
[58.729835, "o", "n"]
[58.868211, "o", " "]
[59.223168, "o", "s"]
[59.582067, "o", "b"]
[59.636177, "o", "a"]
[59.79776, "o", "t"]
[60.061073, "o", "c"]
[60.10449, "o", "h"]
[60.446139, "o", "\r\n\u001b[?2004l\r"]
[60.640138, "o", "\u001b[?1049h\u001b[22;0;0t\u001b[?1h\u001b=\rsbatch(1)                                    Slurm Commands                                   sbatch(1)\u001b[m\r\n\u001b[m\r\n\u001b[1mNAME\u001b[0m\u001b[m\r\n       sbatch - Submit a batch script to Slurm.\u001b[m\r\n\u001b[m\r\n\u001b[1mSYNOPSIS\u001b[0m\u001b[m\r\n       \u001b[1msbatch\u001b[0m [\u001b[4mOPTIONS(0)\u001b[24m...] [ : [\u001b[4mOPTIONS(N)\u001b[24m...]] \u001b[4mscript(0)\u001b[24m [\u001b[4margs(0)\u001b[24m...]\u001b[m\r\n\u001b[m\r\n       Option(s) define multiple jobs in a co-scheduled heterogeneous job.  For more details about hetâ€\u001b[m\r\n       erogeneous jobs see the document\u001b[m\r\n       https://slurm.schedmd.com/heterogeneous_jobs.html\u001b[m\r\n\u001b[m\r\n\u001b[1mDESCRIPTION\u001b[0m\u001b[m\r\n       sbatch submits a batch script to Slurm. The batch script may be given to sbatch through  a  file\u001b[m\r\n       name  on  the  command  line, or if no file name is specified, sbatch will read in a script from\u001b[m\r\n       standard input.\u001b[m\r\n\u001b[m\r\n       The batch script may contain one or more lines beginning with \"#SBATCH\" followed by any  of  the\u001b[m\r\n       CLI  options  documented  on  this  page.  #SBATCH  directives  are  read  directly by Slurm, so\u001b[m\r\n       shell-specific syntax including variable names will be read as  literal  text.  Once  the  first\u001b[m\r\n       non-comment, non-whitespace line has been reached in the script, no more #SBATCH directives will\u001b[m\r\n       be processed. See example below.\u001b[m\r\n\u001b[m\r\n       sbatch exits immediately after the script is successfully transferred to  the  Slurm  controller\u001b[m\r\n       and  assigned a Slurm job ID. The batch script is not necessarily granted resources immediately,\u001b[m\r\n       it may sit in the queue of pending jobs for some  time  before  its  required  resources  become\u001b[m\r\n       available.\u001b[m\r\n\u001b[m\r\n\u001b[7m Manual page sbatch(1) line 1 (press h for help or q to quit)\u001b[27m\u001b[K"]
[63.995969, "o", "\r\u001b[K\u001b[?1l\u001b>\u001b[?1049l\u001b[23;0;0t\u001b[?2004hdane@slurm-login:~$ "]
[65.436742, "o", "\u001b[?2004l\r\r\nlogout\r\n"]
[65.670126, "o", "Shared connection to slurm.ilifu.ac.za closed.\r\r\n"]
[65.67294, "o", "\u001b[?2004h\u001b]0;dane@dane-latitude-7640: ~\u0007\u001b[01;32mdane@dane-latitude-7640\u001b[00m:\u001b[01;34m~\u001b[00m$ "]
[66.860855, "o", "\u001b[?2004l\r\r\nexit\r\n"]
